{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example: Estimating computational requirements for an AI image classification system:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Floating Point Operations (FLOPs) per image\n",
    "flops_per_image = 2 * (1024 * 1024 * 3) * (50 * 10**6)  # ≈ 314 billion\n",
    "\n",
    "# Total FLOPs per day\n",
    "total_flops_per_day = flops_per_image * (1 * 10**6)  # ≈ 314 quadrillion\n",
    "total_flops_per_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Data Integrity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase and remove extra whitespaces\n",
    "    text = ' '.join(text.lower().split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Data Processing and Management**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Generating sample data (replace this with your actual dataset)\n",
    "# Let's assume X is a 2D array representing features\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Creating an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalizing the features\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Displaying the original and normalized data\n",
    "print(\"Original data:\")\n",
    "print(X)\n",
    "print(\"\\nNormalized data:\")\n",
    "print(X_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Best Fit model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Data Preprocessing and Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Perform dimensionality reduction using PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_reduced = pca.fit_transform(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Interpreting model predictions using SHAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values for a single instance\n",
    "shap_values = explainer.shap_values(X_test[0])\n",
    "\n",
    "# Visualize the SHAP values\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Save the preprocessed data\n",
    "preprocessed_data = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "preprocessed_data.to_csv('preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Model Training and Evaluation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Real-time Data Ingestion with Apache Kafka:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction processed successfully - Transaction ID: T1\n",
      "Alert: Fraudulent transaction detected - Transaction ID: T2\n",
      "Transaction processed successfully - Transaction ID: T3\n",
      "Transaction processed successfully - Transaction ID: T4\n",
      "Transaction processed successfully - Transaction ID: T5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_features(transaction):\n",
    "    # Extract relevant features from the transaction\n",
    "    return transaction['amount']\n",
    "\n",
    "def raise_alert(transaction):\n",
    "    # Raise an alert for fraudulent transactions\n",
    "    print(f\"Alert: Fraudulent transaction detected - Transaction ID: {transaction['transaction_id']}\")\n",
    "\n",
    "def process_transaction(transaction):\n",
    "    # Process legitimate transactions\n",
    "    print(f\"Transaction processed successfully - Transaction ID: {transaction['transaction_id']}\")\n",
    "\n",
    "# Load data from the JSON file\n",
    "with open('transactions.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Process incoming data in real-time\n",
    "for transaction in data:\n",
    "    # Extract relevant features from the transaction\n",
    "    amount = extract_features(transaction)\n",
    "    \n",
    "    # Perform model inference (for demonstration purposes, we'll assume fraud if amount > 200)\n",
    "    if amount > 200:\n",
    "        is_fraudulent = 1\n",
    "    else:\n",
    "        is_fraudulent = 0\n",
    "    \n",
    "    # Update the transaction with fraud detection result\n",
    "    transaction['is_fraudulent'] = is_fraudulent\n",
    "    \n",
    "    # Take appropriate action based on the prediction\n",
    "    if is_fraudulent:\n",
    "        # Raise an alert or trigger a fraud prevention mechanism\n",
    "        raise_alert(transaction)\n",
    "    else:\n",
    "        # Process the legitimate transaction\n",
    "        process_transaction(transaction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Encryption and Access Controls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted message: b'gAAAAABmO2-MIl0XqsHaqqv2B2oM4tsLnIruc0Io2W3P1QkaL78vQ8lZxiG8_bXxPwK1VpTjB9fsUO67x4zXbc0bTYJWZ54IuAT0Rdqv71W_UnI7wgjX3V0='\n",
      "Decrypted message: b'This is a secret message!'\n"
     ]
    }
   ],
   "source": [
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# Generate a secure key\n",
    "key = Fernet.generate_key()\n",
    "\n",
    "# Create a Fernet instance with the key\n",
    "fernet = Fernet(key)\n",
    "\n",
    "# Encrypt a message\n",
    "message = b\"This is a secret message!\"\n",
    "encrypted_message = fernet.encrypt(message)\n",
    "print(f\"Encrypted message: {encrypted_message}\")\n",
    "\n",
    "# Decrypt the message\n",
    "decrypted_message = fernet.decrypt(encrypted_message)\n",
    "print(f\"Decrypted message: {decrypted_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
